{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jrVry-tnlYIR",
        "Tl4UPZGuv3P5",
        "rjTrv4Gfyk3f",
        "6mqdCo88860v"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/pdf/1611.01599\n"
      ],
      "metadata": {
        "id": "F1-Dn9AQA9Sf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set up Environment"
      ],
      "metadata": {
        "id": "jrVry-tnlYIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imageio==2.23.0\n",
        "!pip install tensorflow==2.10.1\n",
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "otc6ydBZPWLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uugInvyI_co6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from typing import List\n",
        "from matplotlib import pyplot as plt\n",
        "import imageio # Creates gifs to see frames stacked together\n",
        "from IPython.display import clear_output\n",
        "CROP_HEIGHT = 46\n",
        "CROP_WIDTH = 140"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "rRFDJ71aPuUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "try:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "    print(\"GPU found\")\n",
        "except:\n",
        "    print(\"GPU not found\")"
      ],
      "metadata": {
        "id": "CaANGzguCyxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and preprocess data"
      ],
      "metadata": {
        "id": "QUrMbG9d_6Zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "# Load trained model\n",
        "weights_path = '/content/best.pt'\n",
        "try:\n",
        "    object_detect_model = YOLO(weights_path)\n",
        "    print(f\"Loaded model with best weights from {weights_path}\")\n",
        "except FileNotFoundError:\n",
        "    warnings.warn(f\"MODEL NOT FOUND AT {weights_path}, LOADING GENERIC PRETRAINED YOLOv8 MODEL INSTEAD.\")\n",
        "    object_detect_model = YOLO('yolov8n.pt')  # Load a normal YOLO model with pretrained weights"
      ],
      "metadata": {
        "id": "TcaRmGVPao9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown # Downloads datasets with good compatibility Colab notebooks\n",
        "\n",
        "\n",
        "url = 'https://drive.google.com/uc?id=1YlvpDLix3S-U8fd-gqRwPcWXAXm8JwjL' #More managable subset of very large original dataset\n",
        "output = 'data.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "gdown.extractall('data.zip')"
      ],
      "metadata": {
        "id": "DBbNk73SAhpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_video(path: str, detection_model, crop_height: int, crop_width: int) -> List[float]:\n",
        "    results = detection_model.predict(path, conf=0.5, show=False)\n",
        "    preprocessed_frames = []\n",
        "\n",
        "    for result in results:\n",
        "        if result.boxes.xyxy.shape[0] > 0: # If any detections boxes were detected\n",
        "            for box in result.boxes.xyxy:\n",
        "                x1, y1, x2, y2 = box.int() # Get box coordinates as integers\n",
        "                crop = result.orig_img[y1:y2, x1:x2] # Crop with indices\n",
        "                grey = tf.image.rgb_to_grayscale(crop)\n",
        "                resize = tf.image.resize(grey, (crop_height, crop_width))\n",
        "                preprocessed_frames.append(resize)\n",
        "\n",
        "    frames_tensor = tf.convert_to_tensor(preprocessed_frames, dtype=tf.float32)\n",
        "    mean = tf.math.reduce_mean(frames_tensor)\n",
        "    std = tf.math.reduce_std(frames_tensor)\n",
        "    normalized_frames = (frames_tensor - mean) / std\n",
        "\n",
        "    return normalized_frames"
      ],
      "metadata": {
        "id": "BAktrrLOjsua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VOCABULARY CONVERSION\n",
        "\n",
        "vocab = [x for x in \"abcdefghijklmnopqrstuvwxyz'?!123456789 \"]\n",
        "char_to_num = tf.keras.layers.StringLookup(vocabulary=vocab, oov_token=\"\")\n",
        "num_to_char = tf.keras.layers.StringLookup(\n",
        "    vocabulary=char_to_num.get_vocabulary(), oov_token=\"\", invert=True\n",
        ")\n",
        "\n",
        "print(\n",
        "    f\"Vocab List: {char_to_num.get_vocabulary()}\\n\"\n",
        "    f\"Vocab size: {char_to_num.vocab_size()}\"\n",
        "    )\n"
      ],
      "metadata": {
        "id": "rfyGA0mDK5Jj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(char_to_num(['k', 'e', 'r', 'o']))\n",
        "print(num_to_char([11, 5, 18, 15]))"
      ],
      "metadata": {
        "id": "bY0PcWXwNoaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_labels(path:str) -> List[str]:\n",
        "    tokens = []\n",
        "    with open(path) as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            line = line.split()\n",
        "            if line[2] != 'sil':\n",
        "                tokens.append(' ')\n",
        "                tokens.append(line[2])\n",
        "    return char_to_num(tf.reshape(tf.strings.unicode_split(tokens, input_encoding='UTF-8'), [-1]))"
      ],
      "metadata": {
        "id": "kVL_co3JfqzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(videos_path:str, show_shapes:bool=False): #\n",
        "    videos_path = bytes.decode(videos_path.numpy())\n",
        "    # Get the name of the file to be used for alignment path\n",
        "    file_name = videos_path.split('/')[-1].split('.')[0]\n",
        "    directory = os.path.dirname(os.path.dirname(videos_path))\n",
        "    labels_path = f'{directory}/alignments/s1/{file_name}.align'\n",
        "    frames = load_video(videos_path,\n",
        "                        detection_model=object_detect_model,\n",
        "                        crop_height=CROP_HEIGHT,\n",
        "                        crop_width=CROP_WIDTH,\n",
        "                       )\n",
        "    labels = load_labels(labels_path)\n",
        "    if show_shapes:\n",
        "        print(f'Frames shape:{frames.shape}\\nLabels shape: {labels.shape}')\n",
        "    return frames, labels"
      ],
      "metadata": {
        "id": "4d6kXTIGgtqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths input as tensors for TF Dataset API compatibility and mapping capabilities\n",
        "frames, labels = load_data(tf.convert_to_tensor('/content/data/s1/bbaf2n.mpg'))"
      ],
      "metadata": {
        "id": "oox5llL3niX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_label(labels):\n",
        "    decoded = [bytes.decode(x) for x in num_to_char(labels).numpy()]\n",
        "    return tf.strings.reduce_join(decoded)\n",
        "    #print(tf.strings.reduce_join(decoded).numpy())"
      ],
      "metadata": {
        "id": "tgYnRHlbso8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(frames[np.random.randint(0, frames.shape[0])])\n",
        "print(f\"Sequence: {labels}\\nLabel: {decode_label(labels)}\")"
      ],
      "metadata": {
        "id": "idwXpJsqnnLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_func(path:str)->List[str]:\n",
        "    return tf.py_function(load_data, [path], (tf.float32, tf.int64))"
      ],
      "metadata": {
        "id": "4StYDtxbursw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pipeline"
      ],
      "metadata": {
        "id": "Tl4UPZGuv3P5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = tf.data.Dataset.list_files('/content/data/s1/*.mpg')\n",
        "data = data.shuffle(500, reshuffle_each_iteration=False)\n",
        "data = data.map(map_func)\n",
        "data = data.padded_batch(2, padded_shapes=([75, None, None, None], [40]))\n",
        "data = data.prefetch(tf.data.AUTOTUNE)\n",
        "train = data.take(450)\n",
        "test = data.skip(450)"
      ],
      "metadata": {
        "id": "gaa1gfa_v5_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames, labels = data.as_numpy_iterator().next()\n",
        "plt.imshow(frames[0][0])\n",
        "print(f\"Sequence: {labels[0]} \\n Label: {decode_label(labels[0])}\")"
      ],
      "metadata": {
        "id": "_7GqHNFmwa8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = data.as_numpy_iterator()\n",
        "val = test.next()\n",
        "val[0][0]"
      ],
      "metadata": {
        "id": "aTzztBKAKmW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imageio.mimsave('./example.gif', val[0][0], fps=10)"
      ],
      "metadata": {
        "id": "bday7_0-K1YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Make NN"
      ],
      "metadata": {
        "id": "rjTrv4Gfyk3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vid_shape = data.as_numpy_iterator().next()[0][0].shape"
      ],
      "metadata": {
        "id": "7QXN3iJd1F_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Dropout, TimeDistributed, Flatten, Bidirectional, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "lipnet_model = Sequential([\n",
        "    Conv3D(128, 3, input_shape=vid_shape, padding='same', activation='relu'),\n",
        "    MaxPooling3D((1,2,2)),\n",
        "    Dropout(0.5),\n",
        "    Conv3D(256, 3, padding='same', activation='relu'),\n",
        "    MaxPooling3D((1,2,2)),\n",
        "    Dropout(0.5),\n",
        "    Conv3D(75, 3, padding='same', activation='relu'),\n",
        "    MaxPooling3D((1,2,2)),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    TimeDistributed(Flatten()),\n",
        "\n",
        "    Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)),\n",
        "    Dropout(0.5),\n",
        "    Bidirectional(LSTM(128, kernel_initializer='Orthogonal', return_sequences=True)),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(char_to_num.vocabulary_size()+1, kernel_initializer='he_normal', activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "ylCF08-tz5pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lipnet_model.summary()"
      ],
      "metadata": {
        "id": "l0dStJeg4_wz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat = lipnet_model.predict(val[0])\n",
        "print(f\"Coded:\\n{np.argmax(yhat[0], axis=1)}\\n\")\n",
        "print(f\"Decoded:\\n{decode_label(np.argmax(yhat[0], axis=1))}\")"
      ],
      "metadata": {
        "id": "G5z7hN6s7P7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ],
      "metadata": {
        "id": "6mqdCo88860v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ctc_loss(y_true, y_pred):\n",
        "    batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
        "    input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
        "    label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
        "\n",
        "    input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "    label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
        "    return tf.keras.backend.ctc_batch_cost(y_true, y_pred, input_length, label_length)"
      ],
      "metadata": {
        "id": "G3IADJ9w9PFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lipnet_model.compile(loss=ctc_loss, optimizer=Adam(learning_rate=0.001))"
      ],
      "metadata": {
        "id": "yGnLqyne_pPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pretrained Lipnet Weights\n",
        "url = 'https://drive.google.com/uc?id=1vWscXs4Vt0a_1IH1-ct2TCgXAZT-N3_Y'\n",
        "output = 'checkpoints.zip'\n",
        "gdown.download(url, output, quiet=False)\n",
        "gdown.extractall('checkpoints.zip', 'models')\n",
        "lipnet_model.load_weights('models/checkpoint')"
      ],
      "metadata": {
        "id": "fREz91twDEKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Model (Not required since pretrained weights are loaded)\n",
        "#lipnet_model.fit(train, validation_data=test, epochs=50)"
      ],
      "metadata": {
        "id": "NLjo2HwQAO71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Model"
      ],
      "metadata": {
        "id": "cnn7bUXRUeNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_video(videos_path:str):\n",
        "    video_tensor = load_video(videos_path,\n",
        "                              detection_model=object_detect_model,\n",
        "                              crop_height=CROP_HEIGHT,\n",
        "                              crop_width=CROP_WIDTH,\n",
        "                              )\n",
        "\n",
        "    num_frames = video_tensor.shape[0]\n",
        "    batch_size = 75\n",
        "\n",
        "    # Number of complete batches based on batch size\n",
        "    num_batches = num_frames // batch_size\n",
        "\n",
        "    # Trim video from remainder frames to have a whole number of batches\n",
        "    trimmed_frames = num_batches * batch_size\n",
        "    trimmed_video_tensor = video_tensor[:trimmed_frames]\n",
        "\n",
        "    # Form batches based on batch size and determined number of batches\n",
        "    batches = tf.reshape(trimmed_video_tensor, (num_batches, batch_size, CROP_HEIGHT, CROP_WIDTH, 1))\n",
        "\n",
        "    # Pad remainder frames that were trimmed\n",
        "    remainder_frames = num_frames % batch_size\n",
        "    if remainder_frames > 0:\n",
        "        remainder_video_tensor = video_tensor[-remainder_frames:]\n",
        "        padding_frames = batch_size - remainder_frames\n",
        "        padding_tensor = tf.zeros((padding_frames, CROP_HEIGHT, CROP_WIDTH, 1), dtype=video_tensor.dtype)\n",
        "        padded_remainder_video_tensor = tf.concat([remainder_video_tensor, padding_tensor], axis=0)\n",
        "\n",
        "        # Add the padded batch to the rest of the batches\n",
        "        batches = tf.concat([batches, tf.expand_dims(padded_remainder_video_tensor, axis=0)], axis=0)\n",
        "        imageio.mimsave('./test.gif', batches[0], fps=10)\n",
        "\n",
        "    return batches"
      ],
      "metadata": {
        "id": "xWhbbBeLeKtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_compare(path:str, real_label:bool=True):\n",
        "\n",
        "    if real_label:\n",
        "        sample = load_data(tf.convert_to_tensor(path))\n",
        "        yhat = [lipnet_model.predict(tf.expand_dims(sample[0], axis=0))]\n",
        "    else:\n",
        "        batches = batch_video(path)\n",
        "        yhat = []\n",
        "        for i in range(0,batches.shape[0]-1):\n",
        "            print(f\"Predicting for batch {i+1} of {batches.shape[0]}...\")\n",
        "            yhat.append(lipnet_model.predict(tf.expand_dims(batches[i], axis=0)))\n",
        "\n",
        "\n",
        "    # Decode predictions\n",
        "    clear_output()\n",
        "    predictions_str = \"\"\n",
        "\n",
        "    for prediction in yhat:\n",
        "        decoded = tf.keras.backend.ctc_decode(prediction, input_length=[75], greedy=True)[0][0].numpy()\n",
        "        predictions_str += decode_label(decoded[0]).numpy().decode('utf-8') + \" \"\n",
        "\n",
        "    print(f\"PREDICTIONS:\\n {predictions_str.strip()}\\n\")\n",
        "    if real_label:\n",
        "        print(f\"ACTUAL TEXT:\\n {decode_label(sample[1]).numpy().decode('utf-8').strip()}\")"
      ],
      "metadata": {
        "id": "pw0Yo-8GNl9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Repredicting for train data sample:\n",
        "predict_and_compare('/content/data/s1/bbaz6p.mpg', real_label=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PJTkDAejLap",
        "outputId": "94979c3c-6892-4879-aa1c-a9d80baa5ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREDICTIONS:\n",
            " s ee s ix pleasin\n",
            "\n",
            "ACTUAL TEXT:\n",
            " bin blue at z six please\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting for unseen test data:\n",
        "predict_and_compare('/content/test_video.mp4', real_label=False)\n",
        "actual_text = \"\"\"Uh we didn't meet um until this morning. Um but I watched the France game when\n",
        "I got home. Um I didn't I didn't watch uh our whole game I watched the France game first.\"\"\"\n",
        "print(f\"ACTUAL TEXT:\\n {actual_text}\")"
      ],
      "metadata": {
        "id": "az3csABNXHuq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5efaf255-78d5-4e01-bb39-0e8cd6a0aaeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREDICTIONS:\n",
            " s bue i oe slgain s een it t oue again  een in i sive sgain s bree ni so pslgain\n",
            "\n",
            "ACTUAL TEXT:\n",
            " Uh we didn't meet um until this morning. Um but I watched the France game when \n",
            "I got home. Um I didn't I didn't watch uh our whole game I watched the France game first.\n"
          ]
        }
      ]
    }
  ]
}